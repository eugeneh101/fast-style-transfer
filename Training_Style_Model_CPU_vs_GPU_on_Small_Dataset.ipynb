{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2105    2105   67360\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/train2014/ | wc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data smaller\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "for filename in glob('data/train2014/*'):\n",
    "    if random.random() > 0.025: # use only 2.5% of the data\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn art style: train model\n",
    "# upload steampunk image\n",
    "!time python style.py --style ../steampunk.jpg --checkpoint-dir checkpoint --epochs 2 --checkpoint-iterations 10 --batch-size 16\n",
    "#!nvidia-smi # read \"Disp.A Memory-Usage: and \"GPU Memory Usage\" for how much GPU RAM usage; \"Volatile GPU-Util\" grossly inaccurate\n",
    "!pip install glances # glances is way better than nvidia-smi in terms of accuracy and UI\n",
    "!pip install nvidia-ml-py3\n",
    "!glances\n",
    "# remove test-dir and other parameters for now\n",
    "\n",
    "# use small batches if training pictures are bigger\n",
    "\n",
    "\n",
    "\n",
    "# on 2.5% of pictures and 16 minibatch:\n",
    "# real    9m59.879s\n",
    "# user    9m5.264s\n",
    "# sys     0m48.336s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "!mkdir test_dir\n",
    "!time python style.py --style ../cross_style.jpg --checkpoint-dir checkpoint --epochs 10 --checkpoint-iterations 100 --batch-size 16 --test ../seagull_on_sidewalk_small.jpg --test-dir test_dir\n",
    "\n",
    "#### retry\n",
    "\n",
    "## appears that it will regress to black; maybe GPU RAM constrained?\n",
    "## also uses CPU by default, I changed it to be GPU, don't know if RAM is constrained. Have to make this argparse\n",
    "## every iteration causes checkpoint save (GPU not running) and inference on test image, so use high iteration number\n",
    "\n",
    "# real    56m18.865s\n",
    "# user    50m23.584s\n",
    "# sys     4m13.656s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content weight doubled\n",
    "!mkdir test_dir_content_weight_modified/\n",
    "!time python style.py --style ../cross_style.jpg --checkpoint-dir checkpoint --epochs 10 --checkpoint-iterations 100 --batch-size 16 --test ../seagull_on_sidewalk_small.jpg --test-dir test_dir_content_weight_modified/ --content-weight 1.5e1\n",
    "\n",
    "## appears that it will regress to black; maybe GPU RAM constrained?\n",
    "## also uses CPU by default, I changed it to be GPU, don't know if RAM is constrained. Have to make this argparse\n",
    "## every iteration causes checkpoint save (GPU not running) and inference on test image, so use high iteration number\n",
    "\n",
    "# runtime was just an extra 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate x 10\n",
    "!mkdir test_dir_lr_modified_x10\n",
    "!time python style.py --style ../cross_style.jpg --checkpoint-dir checkpoint --epochs 10 \\\n",
    "    --checkpoint-iterations 100 --batch-size 16 --test ../seagull_on_sidewalk_small.jpg \\\n",
    "    --test-dir test_dir_lr_modified_x10/ --learning-rate 1e-2 &> test_dir_lr_modified_x10/runtime_lr_x10.txt\n",
    "\n",
    "## appears that it will regress to black (False); maybe GPU RAM constrained? (False)\n",
    "## also uses CPU by default, I changed it to be GPU, don't know if RAM is constrained. Have to make this argparse\n",
    "## every iteration causes checkpoint save (GPU not running) and inference on test image, so use high iteration number\n",
    "\n",
    "# real    56m19.089s\n",
    "# user    50m22.908s\n",
    "# sys     4m17.932s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate x 5\n",
    "!mkdir test_dir_lr_modified_x5\n",
    "!time python style.py --style ../cross_style.jpg --checkpoint-dir checkpoint --epochs 10 \\\n",
    "    --checkpoint-iterations 100 --batch-size 16 --test ../seagull_on_sidewalk_small.jpg \\\n",
    "    --test-dir test_dir_lr_modified --learning-rate 5e-3 &> test_dir_lr_modified_x5/runtime_lr_x5.txt\n",
    "    \n",
    "## appears that it will regress to black; maybe GPU RAM constrained?\n",
    "## also uses CPU by default, I changed it to be GPU, don't know if RAM is constrained. Have to make this argparse\n",
    "## every iteration causes checkpoint save (GPU not running) and inference on test image, so use high iteration number\n",
    "\n",
    "# real    45m48.311s\n",
    "# user    42m41.188s\n",
    "# sys     3m3.264s\n",
    "\n",
    "# Learning rate still too high\n",
    "# estimate 6 seconds per checkpoint save and evaluate. ~23% runtime just on this step\n",
    "# possible to isolate time cost by simply not evaluating and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply style on another picture, compare CPU vs GPU\n",
    "# I don't think there are any other important parameters\n",
    "!time python evaluate.py --checkpoint checkpoint --in-path examples/style/rain_princess.jpg --out-path ../output_image.jpg\n",
    "\n",
    "# about the same between GPU (slightly faster) and CPU\n",
    "# real    0m5.287s\n",
    "# user    0m3.556s\n",
    "# sys     0m2.068s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply style to video\n",
    "# upload video: play_dead.mp4\n",
    "# use smaller minibatch if video very high resolution\n",
    "!time python transform_video.py --in-path ../play_dead.mp4   --checkpoint checkpoint   --out-path ../output_video.mp4   --device /gpu:0   --batch-size 16\n",
    "\n",
    "# GPU\n",
    "# real    0m19.682s\n",
    "# user    0m22.548s\n",
    "# sys     0m6.408s\n",
    "\n",
    "# CPU\n",
    "# real    5m50.725s\n",
    "# user    21m21.868s\n",
    "# sys     1m17.100s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first part to GPU to reduce runtime by 22 seconds\n",
    "\n",
    "# Speed\n",
    "## Try: more GPUs\n",
    "## Done: bigger minibatch, less pictures\n",
    "\n",
    "# Quality: figure out the parameters such that improvement plateaus\n",
    "## Try: increase style weight, increase learning rate\n",
    "## Done: add test folder, increase content weight makes fewer iterations to get original picture back\n",
    "\n",
    "# Notes:\n",
    "## notice with more iterations, then color of original image more visible; I assume that means the content loss is being reduced\n",
    "## GPU RAM is full on this EC2 instance by loading up model. Thank goodness it fits into memory! For more powerful GPU, can probably increase minibatch for evaluation\n",
    "## Currently GPU RAM constrained; not sure how it's evaluating testing picture without sufficient GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--style, --checkpoint-dir # explicitly required\n",
    "--train-path, --vgg-path # implicitly required\n",
    "\n",
    "--epochs, --checkpoint-iterations, --batch-size, # optional\n",
    "--test, --test-dir, --content-weight, --style-weight, --tv-weight, --learning-rate # more optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real    763m46.949s\n",
    "user    1192m59.980s\n",
    "sys     74m2.732s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes:\n",
    "4 CPUs at full power vs 1 Tesla k80 (RAM: 12 GB; Compute Capability: 3.7)\n",
    "714 second/iter vs 26 sec/iter   where iter is really 10 iters\n",
    "hatching pattern means still bad, need more iterations to reduce hatching size until it disappears\n",
    "    is hatching an artifact of style image or is it a measure of underfit?\n",
    "takes 1 full epoch for style transfer to be good (5173 iterations); \n",
    "    at 1 epoch 3000 iterations, it probably jumped out local min (however, that's strange because \n",
    "    I think it's using consistent learning rate and not Adam [look at optimizer], so never supposed\n",
    "    to do work)\n",
    "\n",
    "p3 RAM 16 GB; Compute Capability 7.0 -- ~8 sec/iter (386 sec/iter on CPU cuz 8 CPUs)\n",
    "bullion would have substantially more CPUs and more powerful GPU with more RAM, CC 6.0?\n",
    "bullion has a premium GPU. Smaller GPUs like for gaming and bitcoin mining are not large enough to load in a GPU RAM.\n",
    "Our model requires 9 GB of GPU RAM. \n",
    "\n",
    "taskset -p CPU_id PID # constrain process to specific CPU\n",
    "\n",
    "the reason that things are faster is 2 parts: a) persistent GPU RAM which uses more than CPU RAM \n",
    "    (which probably gets garbaged collected) which uses only around 6 GB RAM\n",
    "    b) lots more cores in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase interation count per checkpoint\n",
    "try to train with given style and given content (they use full dataset)\n",
    "use p3 instance to train rapidly; use p2.8x to do simultaneous model (probably requires code change)\n",
    "write script to detect new model and rename it to keep it from being overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "upload all the images\n",
    "make checkpoint and test_dir \n",
    "made small change to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/style-transfer-installation/Installing_TensorFlow_with_CUDA.sh\n",
    "!time bash Installing_TensorFlow_with_CUDA.sh\n",
    "\n",
    "# delete ffmpeg installation: conda install -y ffmpeg -c conda-forge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
